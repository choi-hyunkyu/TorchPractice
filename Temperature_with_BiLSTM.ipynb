{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Temperature_with_BiLSTM.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choi-hyunkyu/TorchPractice/blob/choi-hyunkyu-patch-1/Temperature_with_BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGXw0dWAsKZF"
      },
      "source": [
        "#### <center>국민대학교 소프트웨어 융합대학원 인공지능전공 최현규 K2020515</center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOsEhkvksKZG"
      },
      "source": [
        "# 1. Time Series Analysis\n",
        "\n",
        "## 1-1. 시계열 - TimeSeries\n",
        "\n",
        "* 시계열(time series)란 일정 시간 간격(연도별, 분기별, 월별, 일별, 시간별 등)으로 시간의 경과(흐름)에 따라 관측되는 자료를 말한다. 시계열들은 생성되는 특성에 따라 연속적으로 생성되는 연속시계열과 이산적 시점에서 생성되는 이산시계열로 구분할 수 있다.\n",
        "\n",
        "\n",
        "* 시계열 분석(time series analysis)라고 하는 것은 시계열을 해석하고 이해하는 데 쓰이는 여러 가지 방법을 연구하는 분야를 말하며, 시계열 예측(time series prediction)이라고 하는 것은 주어진 시계열에 대해 수학적인 모델을 만들고, 과거 시계열이 미래에 일어날 사건을 예측하는 것을 말한다.\n",
        "\n",
        "\n",
        "* 딥러닝 연구분야에서 가장 기본적으로 시도되는 분야가 시계열 분석(time series analysis)이며 데이터의 양이 이미지, 음성, 영상 데이터의 양에 비해 상대적으로 작기 때문이다.\n",
        "\n",
        "## 1-2. 시계열 분석 기법\n",
        "\n",
        "* AR(AutoreRression) 모델: 자기 회귀 모델이라고 불리는 모델이며, 이전의 과거 데이터를 사용하여 회귀한다. 과거의 데이터가 미래의 데이터에 영향을 준다는 점에서 RNN(Recurrent Neural Network)와 비슷하다.\n",
        "\n",
        "\n",
        "* MA(Moving Average) 모델: 이동 평균 모델이라고 불리는 모델이며, 트렌드(Trend, 평균 혹은 시계열 그래프에서 y값)가 변화하는 상황에서 적합한 회귀 모델이다.\n",
        "\n",
        "\n",
        "* ARMA(AutoRegression Moving Average) 모델: AR 모델과 MA 모델을 결합한 모델이다.\n",
        "\n",
        "\n",
        "* ARIMA(AutoRegression Integrated Moving Average): ARMA에서 Integrated 개념을 추가한 모델이다. 뷸규칙한 시계열 데이터(소량의 데이터)를 분석하는 모델이다.\n",
        "\n",
        "\n",
        "* Deep Learning(LSTM): LSTM은 RNN의 변형된 종류로, 시퀀스가 긴 데이터를 분석할 때 매우 용이한 방법이다.\n",
        "\n",
        "\n",
        "* 위의 방법들 중에서 딥러닝을 적용하여 과거의 온도 데이터를 통해서 미래의 온도 데이터를 예측해보는 프로젝트를 진행하겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiAvPvdSsKZG"
      },
      "source": [
        "# 2. Pytorch\n",
        "\n",
        "## 2-1. Pytorch란?\n",
        "\n",
        "* Pytorch는 2017년에 공개된 딥러닝 프레임워크로 개발자, 연구자들이 GPU를 활용하여 인공 신경망을 만들고 딥러닝 연구를 용이하게 할 수 있게 만든 Python 기반 라이브러리이다.\n",
        "\n",
        "\n",
        "* Pytorch는 Facebook의 인공지능 연구부서에서 관리하며 독자적으로 운영되는 Pytorch Forum은 사람들이 질문을 던지면 여러 사용자는 물론 연구부서에서도 답글을 게시할 만큼 활발하게 운영되고 있다.\n",
        "\n",
        "## 2-2. Pytorch와 다른 FrameWork의 비교\n",
        "\n",
        "* Pytorch는 기본적으로 Numpy를 사용한다. x, y, z 세 변수에 대해 학습하는 과정을 예로 들면, 신경망 학습을 할 때 기울기를 계산하기 위해서는 연산 그래프를 따라서 미분해야한다. Numpy만을 사용한다면 모든 미분 식을 직접 계산하고 코드로 작성해야 하므로 변수 하나당 두 줄씩 필요하게 되므로 총 여섯 줄이 필요하게 된다. 반면 Pytorch를 사용하게 되면 backward() 함수를 통해 자동으로 연산이 가능하기 때문에 간편하다는 장점이 있다.\n",
        "\n",
        "\n",
        "* Numpy를 사용하는 것 이외에 또 다른 장점은 GPU의 사용여부에 있다. Numpy만으로는 GPU로 값을 내보내고 다시 돌려받는 것이 불가능하다. 반면 Pytorch는 CUDA, cuDNN이라는 API를 통해 GPU 연산을 할 수 있다. cuDNN은 CUDA를 이용해 딥러닝 연산을 가속해주는 라이브러리이다. 병렬 연산에서 GPU의 속도는 CPU 속도보다 월등히 빠르며 CUDA와 cuDNN을 동시에 사용할 경우 연산 속도가 CPU의 15배 이상이 된다고 알려져 있다. 심층 신경망을 정의할 때 함수와 기울기 계산 그리고 GPU를 이용한 연산 가속 등의 장점이 우수하기 때문에 딥러닝 연구 시 GPU의 사용 여부는 필수적이라고 할 수 있다.\n",
        "\n",
        "\n",
        "* Pytorch와 Tensorflow는 모두 GPU를 사용하는 딥러닝 프레임워크이다. 차이점이 있다면 Pytorch는 그래프를 만듦과 동시에 값이 할당되는 'Define by Run' 방식이고, Tensorflow는 연산 그래프를 먼저 만들고 실제 연산할 때 값을 전달하여 결과를 얻는 'Define and Run' 방식이다.\n",
        "\n",
        "    Pytorch : Define by Run | Tensorflow : Define and Run\n",
        "\n",
        "\n",
        "* 주로 연구목적으로는 Pytorch, 현업에서는 Tensorflow를 주로 사용하고 있다고 알려져있다.\n",
        "\n",
        "출처: https://jfun.tistory.com/238"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SfPVN3HsKZG"
      },
      "source": [
        "# 3 신경망 모델의 종류\n",
        "\n",
        "## 3-1 RNN\n",
        "\n",
        "* 가장 기본적인 MLP 모델의 경우 전부 은닉층에서 활성화 함수(activation function)를 지난 값은 오직 출력층 방향으로만 향한다. 이와 같은 신경망들을 피드 포워드 신경망(Feed Forward Neural Network)라고 한다. 이와 별개인 모델 중에서 RNN이라는 모델이 있다.\n",
        "\n",
        "\n",
        "* RNN(Recurrent Neural Network)은 시퀀스(Sequence) 모델이다. 입력과 출력을 시퀀스 단위로 처리하는 모델이며, 번역을 예로 들었을 때 입력은 번역하고자 하는 문장(시퀀스)이다. 출력에 해당되는 번역된 문장 또한 단어 시퀀스이다. 이러한 시퀀스를 처리하기 위해 고안된 모델을 시퀀스 모델이라고 한다. 그 중에서 RNN은 딥러닝에 있어서 가장 기본적인 시퀀스 모델이다.\n",
        "\n",
        "\n",
        "* RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로 보내면서 은닉층 노드의 다음 입력으로 전달하는 특징이 있다. 이를 그림으로 표현한 것이 아래와 같다. x는 입력층의 입력 벡터, y는 출력층의 출력 벡터이다(편향 b는 그림에서 생략되어 있음). RNN은 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 셀(Cell)이라고 한다. 셀은 과거의 값을 기억하는 일종의 메모리 역할을 수항하기 때문에 메모리 셀 또는 RNN 셀이라고 표현된다.\n",
        "\n",
        "\n",
        "* 피드 포워드 신경망에서는 뉴런이라는 단위를 주로 사용하지만 RNN에서는 뉴런이라는 단위보다는 입력층과 출력층에서는 입력 벡터와 출력벡터, 은닉층에서는 은닉 상태라는 표현을 주로 사용한다.\n",
        "\n",
        "\n",
        "* RNN은 입력과 출력의 길이를 다르게 설계할 수 있으므로 다양한 용도로 사용될 수 있다. 하나의 입력에 대해서 여러 개의 출력(one-to-many)의 모델은 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝(Image Captioning) 작업에 사용할 수 있다. 또한 단어 시퀀스에 대해서 하나의 출력(may-to-one)의 모델은 입력 문서가 긍정적인지 부정적인지를 판별하는 감성 분류(sentiment classification) 등에 사용될 수 있다. 다 대 다(many-to-many)의 모델의 경우에는 입력 문장으로 부터 대답 문장을 출력하는 챗봇과 입력 문장으로부터 번역된 문장을 출력하는 번역기, 개체명 인식이나 품사 태깅과 같은 작업들에 사용될 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SkYXb7_uqsm"
      },
      "source": [
        "from google.colab import files\n",
        "original_data = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q55e1MuwANW"
      },
      "source": [
        "#file.download('RNN.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0DA6moGsKZG"
      },
      "source": [
        "# RNN\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image = cv2.imread('RNN.png', cv2.IMREAD_UNCHANGED)\n",
        "plt.imshow(image)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y5EPQBvsKZG"
      },
      "source": [
        "## 3-2 LSTM\n",
        "\n",
        "* RNN은 기본적으로 출력이 이전의 계산 결과에 의존하게 된다. 다시 말하면, RNN의 시점(time step)이 길어질 수록 과거의 정보가 미래로 충분히 전달되지 못하는 현상이 발생한다는 것이다. 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 한다. 이러한 장기 의존성 문제를 해결하기 위해 RNN의 단점을 보완한 장단기 메모리(Long Short Term Memory)-RNN이 개발되었으며 이를 LSTM이라고 부른다. LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 메모리를 지우고 기억해야할 것을 정한다.\n",
        "\n",
        "\n",
        "* $h_t = tanh(W_x  x_t + W_h  h_(t-1) + b)$\n",
        "\n",
        "\n",
        "* 입력, 출력과 셀을 수식으로 표현하면 위와 같다. $x_t$와 $h_(t-1)$이라는 두 개의 입력이 각각의 가중치와 곱해진 후 메모리의 입력이 된다. 이 값은 은닉층의 출력인 은닉 상태가 된다. 아래의 그림은 LSTM 내부의 전체적인 그림이다. LSTM은 은닉 상태(hidden state)를 계산하는 방법이 기존의 RNN보다 조금 더 복잡하며 셀 상태(cell state)라는 값이 추가되었다. 아래의 그림에서는 t 시점의 셀 상태를 $C_t$로 표현한다. LSTM은 기존의 RNN과 비교하여 긴 시퀀스(Long Sequence)의 입력을 계산하는 방법에 있어서 탁월한 성능을 보인다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPblcb9ou8l9"
      },
      "source": [
        "from google.colab import files\n",
        "original_data = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNccTB8ywHwW"
      },
      "source": [
        "#file.download('LSTM.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54WnySlsKZG"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image = cv2.imread('LSTM.png', cv2.IMREAD_UNCHANGED)\n",
        "plt.imshow(image)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN9rz2C7sKZG"
      },
      "source": [
        "# 4. scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYOOICRisKZH"
      },
      "source": [
        "* scikit-learn이란 python을 대표하는 머신러닝 라이브러리로 '사이킷런'이라고 부르기도 한다. 기본적으로 오픈소스이며 개인, 비즈니스, 등 관계없이 누구나 무료로 사용가능하다. 현재도 개발과 업데이트가 활발하게 이루어지고 있으며 많은 사람들이 이용하기 때문에 관련 정보도 얻기가 용이하다.\n",
        "\n",
        "\n",
        "* 현재 프로젝트에서는 Pytorch를 이용하여 직접 구현하므로 scikit-learn에는 많은 함수 기능이 있지만 preprocessing 부분에서 scaler 부분만 다루도록 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQd0n6RvsKZH"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPysg7TcsaaG"
      },
      "source": [
        "from google.colab import files\n",
        "original_data = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6DYMzrvwNn9"
      },
      "source": [
        "#file.download('original_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxaGQR-psKZH"
      },
      "source": [
        "'''\n",
        "필요한 모듈 임포트\n",
        "'''\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CW1ppvusKZH"
      },
      "source": [
        "'''\n",
        "데이터 불러오기\n",
        "\n",
        "데이터는 기상청 기상자료개방포털에서 서울지역의 1954년부터 2020년까지의 데이터를 일 단위로 다운로드했다\n",
        "\n",
        "기상청 기상자료개방포털(747KB) - https://data.kma.go.kr/data/grnd/selectAsosRltmList.do?pgmNo=36\n",
        "\n",
        "연도 부분에서 1900년도와 2000년도를 구분하기 위해 천의 자리, 백의 자리 / 십의 자리 일의 자리를 구분하여 \n",
        "각각 frontyear, backyear로 column 이름을 지정한 후, original_data.csv로 저장한다\n",
        "'''\n",
        "original_data_df = pd.read_csv(io.BytesIO(original_data['original_data.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbibobf_sKZH"
      },
      "source": [
        "original_data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcCuZY5sKZH"
      },
      "source": [
        "original_data_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP6bdRSasKZH"
      },
      "source": [
        "'''\n",
        "해당 기상자료 데이터는 서울지역에 국한된 데이터이기 때문에 \n",
        "시계열 분석에서 의미가 없는 데이터이므로 삭제한다\n",
        "'''\n",
        "data_df = original_data_df.drop(['location'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyXH4BWusKZH"
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WddQQ-QtsKZH"
      },
      "source": [
        "data_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWGPd9wOsKZH"
      },
      "source": [
        "'''\n",
        "전처리, 신경망 학습 과정에서 결측치가 존재하면 오류가 생기기 때문에 \n",
        "확인한 후 다른 값으로 변경하거나 해당 행을 제거한다\n",
        "'''\n",
        "data_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i68f5oksKZH"
      },
      "source": [
        "'''\n",
        "결측치는 시계열데이터이기 때문에 바로 앞과 뒤의 데이터는 \n",
        "크게 차이가 나지 않는다는 점을 생각해서 결측치를 바로 앞 데이터로 대체한다\n",
        "'''\n",
        "data_df = data_df.fillna(method = 'pad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27mES6wjsKZH"
      },
      "source": [
        "data_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y33c3sNEsKZH"
      },
      "source": [
        "'''\n",
        "데이터프레임의 데이터를 수치적으로 계산하기 위해 numerical data로 변경한다\n",
        "'''\n",
        "data_df = data_df.apply(pd.to_numeric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjbOPJaRsKZH"
      },
      "source": [
        "'''\n",
        "각 계절을 구분하고 가중치를 두기 위해서 각 계절 별 데이터를 추가한다\n",
        "\n",
        "Month column에 있는 데이터를 별도로 데이터프레임으로 저장한다\n",
        "\n",
        "Spring: 3 ~ 5 - 0.05\n",
        "Summer: 6 ~ 8 - 0.25\n",
        "Fall: 9 ~ 11 - 0.42\n",
        "Winter: 12 ~ 2 - 0.28\n",
        "'''\n",
        "season_data_df = pd.DataFrame(data_df['month'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4M4pfrYsKZH"
      },
      "source": [
        "season_data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQZ2BCZdsKZH"
      },
      "source": [
        "'''\n",
        "column의 이름을 season으로 변경한다\n",
        "'''\n",
        "season_data_df.columns = ['season']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZpjLliUsKZH"
      },
      "source": [
        "season_data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjlkOOw3sKZH"
      },
      "source": [
        "'''\n",
        "기존의 데이터프레임과 별도로 구분한 season column 데이터프레임을 결합한다\n",
        "'''\n",
        "new_data_df = pd.concat([data_df, season_data_df], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faS9ziJKsKZH"
      },
      "source": [
        "new_data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy4L9PAasKZH"
      },
      "source": [
        "new_data_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Zg6qPdsKZH"
      },
      "source": [
        "'''\n",
        "계절 별 데이터를 정해진 수치별로 클러스터링한다\n",
        "'''\n",
        "for season in new_data_df:\n",
        "    new_data_df.loc[(new_data_df['season'] >= 1) & (new_data_df['season'] < 3), 'season'] = 0.05\n",
        "    new_data_df.loc[(new_data_df['season'] >= 3) & (new_data_df['season'] < 6), 'season'] = 0.25\n",
        "    new_data_df.loc[(new_data_df['season'] >= 6) & (new_data_df['season'] < 9), 'season'] = 0.42\n",
        "    new_data_df.loc[(new_data_df['season'] >= 9) & (new_data_df['season'] < 12), 'season'] = 0.28\n",
        "    new_data_df.loc[(new_data_df['season'] >= 12), 'season'] = 0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaJMdAwFsKZH"
      },
      "source": [
        "new_data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS0LTn7zsKZH"
      },
      "source": [
        "'''\n",
        "scikit-learn의 MaxAbsScaler() 함수를 scaler 변수로 저장한다\n",
        "'''\n",
        "scaler = MaxAbsScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9vF2yTBsKZH"
      },
      "source": [
        "'''\n",
        "scalering을 진행하게되면 데이터프레임의 column 이름이 초기화되기 때문에 별도로 다시 이름을 지정한다\n",
        "'''\n",
        "scaled_data_df = pd.DataFrame(scaler.fit_transform(new_data_df))\n",
        "scaled_data_df.columns = ['frontyear', 'backyear', 'month', 'day', 'temp_avg', 'temp_min', 'temp_max', 'season']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-7m958vsKZI"
      },
      "source": [
        "scaled_data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egj5EeGgsKZI"
      },
      "source": [
        "scaled_data_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeE1c1FksKZI"
      },
      "source": [
        "'''\n",
        "train, validation, test data를 분리한다\n",
        "'''\n",
        "total_size = int(len(scaled_data_df))\n",
        "\n",
        "train_data_df = scaled_data_df[0:int(total_size * 0.7)].reset_index(drop = True)\n",
        "validation_data_df = scaled_data_df[:int(total_size * 0.15):].reset_index(drop = True)\n",
        "test_data_df = scaled_data_df[int(total_size * 0.85):].reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Il0KMoCsKZI"
      },
      "source": [
        "'''\n",
        "분리된 train, validation, test data의 크기를 확인한다\n",
        "'''\n",
        "print(\"Total Length: {}\".format(len(scaled_data_df)))\n",
        "print(\"Train Length: {} | Validation Length: {} | Test Length: {}\".format(len(train_data_df), len(validation_data_df), len(test_data_df)))\n",
        "print(\"{} + {} + {} = {}\".format(len(train_data_df), len(validation_data_df), len(test_data_df), len(train_data_df) + len(validation_data_df) + len(test_data_df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdmKGoZ2sKZI"
      },
      "source": [
        "'''\n",
        "data의 shape를 확인한다\n",
        "'''\n",
        "print(\"Total Length: {}\".format(total_size))\n",
        "print(\"Total DataFrame Shape: {}\".format(scaled_data_df.shape))\n",
        "print(\"Train DataFrame Shape: {}\".format(train_data_df.shape))\n",
        "print(\"Validation DataFrame Shape: {}\".format(validation_data_df.shape))\n",
        "print(\"Test DataFrame Shape: {}\".format(test_data_df.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWYTBgUysKZI"
      },
      "source": [
        "'''\n",
        "train, validation, test data를 input, target data로 분리한다\n",
        "'''\n",
        "input_feature_list = ['frontyear', 'backyear', 'month', 'day', 'season']\n",
        "target_feature_list = ['temp_avg', 'temp_min', 'temp_max']\n",
        "\n",
        "x_train_data_df = train_data_df[input_feature_list]\n",
        "y_train_data_df = train_data_df[target_feature_list]\n",
        "x_validation_data_df = validation_data_df[input_feature_list]\n",
        "y_validation_data_df = validation_data_df[target_feature_list]\n",
        "x_test_data_df = test_data_df[input_feature_list]\n",
        "y_test_data_df = test_data_df[target_feature_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLGXy6mhsKZI"
      },
      "source": [
        "'''\n",
        "분리된 train, validation, test의 input, target data shape를 확인한다\n",
        "'''\n",
        "print(\"X Train DataFrame Shape: {}\".format(x_train_data_df.shape))\n",
        "print(\"Y Train DataFrame Shape: {}\".format(y_train_data_df.shape))\n",
        "print(\"X Validation DataFrame Shape: {}\".format(x_validation_data_df.shape))\n",
        "print(\"Y Validation DataFrame Shape: {}\".format(y_validation_data_df.shape))\n",
        "print(\"X Test DataFrame Shape: {}\".format(x_test_data_df.shape))\n",
        "print(\"Y Test DataFrame Shape: {}\".format(y_test_data_df.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kokryhkmsKZI"
      },
      "source": [
        "'''\n",
        "GPU 사용 여부를 확인한다\n",
        "'''\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwT-b4OAsKZJ"
      },
      "source": [
        "print(\"{} has been operated\".format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3g81gh8sKZJ"
      },
      "source": [
        "'''\n",
        "프로그래밍된 코드를 일관된 데이터로써 확인하기 위해 랜덤시드를 고정한다\n",
        "'''\n",
        "torch.manual_seed(515)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzPNKE8DsKZJ"
      },
      "source": [
        "'''\n",
        "하이퍼파라미터를 정의한다\n",
        "\n",
        "input shape : (batch size, sequence length, input dimension)\n",
        "'''\n",
        "num_years = 21\n",
        "batch_size = 12 * num_years # 21years data every batch\n",
        "sequence_length = 1\n",
        "input_size = 5 # input data has five features\n",
        "hidden_size = 32\n",
        "num_layers = 3\n",
        "output_size = 3 # output data has three features\n",
        "learning_rate = 1e-5\n",
        "max_norm = 5 # gradient clipping\n",
        "nb_epochs = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY8UC-FIsKZJ"
      },
      "source": [
        "'''\n",
        "dataset function를 정의한다\n",
        "'''\n",
        "def MakeDataSet(x_data_df, y_data_df):\n",
        "    x_ts = torch.FloatTensor(np.array(x_data_df))\n",
        "    y_ts = torch.FloatTensor(np.array(y_data_df))\n",
        "    dataset_ts = TensorDataset(x_ts, y_ts)\n",
        "    return dataset_ts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grWfstGPsKZJ"
      },
      "source": [
        "'''\n",
        "dataloader function를 정의한다\n",
        "'''\n",
        "def MakeDataLoader(dataset, batch_size):\n",
        "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
        "    return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8ZJKwkfsKZJ"
      },
      "source": [
        "'''\n",
        "dataset을 구성한다\n",
        "'''\n",
        "train_dataset_ts = MakeDataSet(x_train_data_df, y_train_data_df)\n",
        "validation_dataset_ts = MakeDataSet(x_validation_data_df, y_validation_data_df)\n",
        "test_dataset_ts = MakeDataSet(x_test_data_df, y_test_data_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJC2HLFRsKZJ"
      },
      "source": [
        "'''\n",
        "dataloader를 구성한다\n",
        "'''\n",
        "train_dataloader = MakeDataLoader(train_dataset_ts, batch_size)\n",
        "validation_dataloader = MakeDataLoader(validation_dataset_ts, batch_size)\n",
        "test_dataloader = MakeDataLoader(test_dataset_ts, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFa4c4M_sKZJ"
      },
      "source": [
        "'''\n",
        "사용될 데이터의 일부분을 확인한다\n",
        "'''\n",
        "for index, value in enumerate(train_dataloader):\n",
        "    while index < 6:\n",
        "        x, y = value\n",
        "        print(\"{} Batch\".format(index))\n",
        "        print(\"Input: {}\".format(x.shape))\n",
        "        print(\"Target: {}\".format(y.shape))\n",
        "        \n",
        "        break    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02t_1vr3sKZJ"
      },
      "source": [
        "'''\n",
        "모델의 양방향 LSTM layer 를 정의한다\n",
        "\n",
        "'''\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = self.input_size,\n",
        "            hidden_size = self.hidden_size,\n",
        "            num_layers = self.num_layers,\n",
        "            dropout = 0.3,\n",
        "            batch_first = True,\n",
        "            bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(\n",
        "            in_features = hidden_size * 2, \n",
        "            out_features = output_size, \n",
        "            bias = True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # init hidden and cell state\n",
        "        hidden_state_0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
        "        cell_state_0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
        "        \n",
        "        # forward pass\n",
        "        out, _ = self.lstm(x, (hidden_state_0, cell_state_0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yAPVh6osKZJ"
      },
      "source": [
        "'''\n",
        "모델, 비용함수, 옵티마이저를 구성한다\n",
        "'''\n",
        "model = BiLSTM(input_size, hidden_size, output_size, num_layers).to(device)\n",
        "criterion = nn.MSELoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm44KdausKZJ"
      },
      "source": [
        "print(model)\n",
        "print(criterion)\n",
        "print(optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU8TmUzCsKZJ"
      },
      "source": [
        "'''\n",
        "구성된 데이터, 모델, 비용함수, 옵티마이저를 통해 테스트를 진행한다\n",
        "'''\n",
        "x, y = list(train_dataloader)[0]\n",
        "x = x.view(-1, sequence_length, input_size).to(device)\n",
        "y = y.to(device)\n",
        "hypothesis = model(x)\n",
        "loss = criterion(hypothesis, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "UB0lblQMsKZJ"
      },
      "source": [
        "print(\"x: {} | {} | x demension: {}\".format(x[0], x.shape, x.dim()))\n",
        "print(\"y: {} | {} | y demension: {}\".format(y[0], y.shape, y.dim()))\n",
        "print(\"hypothesis: {} | {}\".format(hypothesis.shape, hypothesis.dim()))\n",
        "print(\"loss: {}\".format(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWEcLM-2zUZ6"
      },
      "source": [
        "'''\n",
        "학습을 진행한다\n",
        "'''\n",
        "trn_loss_list = []\n",
        "val_loss_list = []\n",
        "for epoch in range(nb_epochs):\n",
        "    \n",
        "    # Train\n",
        "    trn_loss = 0.0\n",
        "    for i, train_samples in enumerate(train_dataloader):\n",
        "        \n",
        "        # train data setting\n",
        "        x_train, y_train = train_samples\n",
        "        x_train = x_train.view(-1, sequence_length, input_size).to(device)\n",
        "        y_train = y_train.to(device)\n",
        "        \n",
        "        # train\n",
        "        model.train()\n",
        "        hypothesis = model(x_train)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = criterion(hypothesis, y_train)\n",
        "        train_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # train loss\n",
        "        trn_loss += train_loss.item() / len(train_dataloader)\n",
        "    \n",
        "    trn_loss_list.append(trn_loss)\n",
        "    \n",
        "    # Evaluation\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        for j, validation_samples in enumerate(validation_dataloader):\n",
        "            \n",
        "            # validatoin data setting\n",
        "            x_validation, y_validation = validation_samples\n",
        "            x_validation = x_validation.view(-1, sequence_length, input_size).to(device)\n",
        "            y_validation = y_validation.to(device)\n",
        "            \n",
        "            # evaluation\n",
        "            model.eval()\n",
        "            prediction = model(x_validation)\n",
        "            validation_loss = criterion(prediction, y_validation)\n",
        "            \n",
        "            # validation loss\n",
        "            val_loss += validation_loss.item() / len(validation_dataloader)\n",
        "    val_loss_list.append(val_loss)\n",
        "    \n",
        "    print(\"Epoch: {:3d} | Train Loss: {:.6f} | Val Loss: {:.6f}\".format(epoch + 1, trn_loss, val_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMaVhiSpsKZJ"
      },
      "source": [
        "'''\n",
        "train, validation loss를 확인한다\n",
        "'''\n",
        "print(\"train loss list length:\", len(trn_loss_list))\n",
        "print(\"validation loss list length:\", len(val_loss_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkBZ3bg0sKZJ"
      },
      "source": [
        "'''\n",
        "train, validation loss를 시각화한다\n",
        "'''\n",
        "plt.figure(figsize = (16, 9))\n",
        "plt.plot(trn_loss_list, label = 'Train Loss')\n",
        "plt.plot(val_loss_list, label = 'Validation Loss')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WklOQEJsKZJ"
      },
      "source": [
        "'''\n",
        "test를 진행한다\n",
        "'''\n",
        "original = []\n",
        "result = []\n",
        "for i, batch in enumerate(test_dataloader):\n",
        "    x, y = batch\n",
        "    x = x.view(-1, sequence_length, input_size).to(device)\n",
        "    y = y.to(device)\n",
        "    pred = model(x)\n",
        "    label = y\n",
        "    loss = criterion(pred, label)\n",
        "    original.append(y.tolist())\n",
        "    result.append(pred.tolist())\n",
        "    \n",
        "print(len(result))\n",
        "print(len(original))\n",
        "\n",
        "test_original_np = np.array(sum(sum(original, []), []))\n",
        "test_result_np = np.array(sum(sum(result, []), []))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGjaqvb0sKZJ"
      },
      "source": [
        "'''\n",
        "reshape dataframe 확인\n",
        "'''\n",
        "test_original_df = pd.DataFrame(test_original_np.reshape(-1, 3))\n",
        "test_result_df = pd.DataFrame(test_result_np.reshape(-1, 3))\n",
        "\n",
        "print(test_original_df.shape)\n",
        "print(test_result_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByaC4GKYsKZJ"
      },
      "source": [
        "'''\n",
        "결과 데이터를 결합한다\n",
        "'''\n",
        "reshaped_test_original_df = pd.concat([x_test_data_df, test_original_df], axis = 1)\n",
        "reshaped_test_result_df = pd.concat([x_test_data_df, test_result_df], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j5_ixBZsKZJ"
      },
      "source": [
        "'''\n",
        "데이터를 결합할 때 season column이 중간에 결합되었다\n",
        "\n",
        "scikit-learn의 inverse transform을 진행하기 위해서는 데이터의 구성이 동일해야하기 때문에 season column의 위치를 변경한다\n",
        "'''\n",
        "reshaped_test_original_df.info\n",
        "reshaped_test_original_df.columns = ['frontyear', 'backyear', 'month', 'day', 'season', 'O_temp_avg', 'O_temp_min', 'O_temp_max']\n",
        "reshaped_test_result_df.columns = ['frontyear', 'backyear', 'month', 'day', 'season', 'R_temp_avg', 'R_temp_min', 'R_temp_max']\n",
        "\n",
        "reshaped_test_original_df = reshaped_test_original_df[['frontyear', 'backyear', 'month', 'day', 'O_temp_avg', 'O_temp_min', 'O_temp_max', 'season']]\n",
        "reshaped_test_result_df = reshaped_test_result_df[['frontyear', 'backyear', 'month', 'day', 'R_temp_avg', 'R_temp_min', 'R_temp_max', 'season']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kugsX0Y1sKZJ"
      },
      "source": [
        "print(reshaped_test_original_df.head())\n",
        "print(reshaped_test_result_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPnrIp3vsKZJ"
      },
      "source": [
        "'''\n",
        "예측된 데이터프레임에 inverse transform을 진행한다\n",
        "'''\n",
        "inversed_test_original_np = scaler.inverse_transform(reshaped_test_original_df)\n",
        "inversed_test_original_df = pd.DataFrame(inversed_test_original_np)\n",
        "\n",
        "inversed_test_result_np = scaler.inverse_transform(reshaped_test_result_df)\n",
        "inversed_test_result_df = pd.DataFrame(inversed_test_result_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asLglOd1sKZJ"
      },
      "source": [
        "'''\n",
        "scikit-learn의 scaler를 사용하면 데이터프레임의 column 이름이 초기화되기 때문에 데이터프레임의 column 이름을 다시 지정한다\n",
        "'''\n",
        "inversed_test_original_df.columns = [['frontyear', 'backyear', 'month', 'day', 'O_temp_avg', 'O_temp_min', 'O_temp_max', 'season']]\n",
        "inversed_test_result_df.columns = [['frontyear', 'backyear', 'month', 'day', 'P_temp_avg', 'P_temp_min', 'P_temp_max', 'season']]\n",
        "\n",
        "dropped_test_original_df = inversed_test_original_df[['O_temp_avg', 'O_temp_min', 'O_temp_max']]\n",
        "dropped_test_result_df = inversed_test_result_df[['P_temp_avg', 'P_temp_min', 'P_temp_max']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkRp51ahsKZJ"
      },
      "source": [
        "\n",
        "'''\n",
        "결과 데이터를 시각화한다\n",
        "'''\n",
        "\n",
        "# average\n",
        "print(\"Average temperature\")\n",
        "plt.figure(figsize = (16, 9))\n",
        "plt.plot(dropped_test_original_df[['O_temp_avg']], label = 'Original')\n",
        "plt.plot(dropped_test_result_df[['P_temp_avg']], label = 'Prediction')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s6KVum7sKZJ"
      },
      "source": [
        "# maximum\n",
        "print(\"Maximum temperature\")\n",
        "plt.figure(figsize = (16, 9))\n",
        "plt.plot(dropped_test_original_df[['O_temp_max']], label = 'Original')\n",
        "plt.plot(dropped_test_result_df[['P_temp_max']], label = 'Prediction')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mGII8KSsKZJ"
      },
      "source": [
        "# minimum\n",
        "print(\"Minimum temperature\")\n",
        "plt.figure(figsize = (16, 9))\n",
        "plt.plot(dropped_test_original_df[['O_temp_min']], label = 'Original')\n",
        "plt.plot(dropped_test_result_df[['P_temp_min']], label = 'Prediction')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW16Hkx5sKZJ"
      },
      "source": [
        "# average[Ni:Nj]\n",
        "print(\"Average temperature\")\n",
        "plt.figure(figsize = (16, 9))\n",
        "plt.plot(dropped_test_original_df[['O_temp_avg']][150:200], label = 'Original')\n",
        "plt.plot(dropped_test_result_df[['P_temp_avg']][150:200], label = 'Prediction')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stcZ08uvsKZK"
      },
      "source": [
        "# 결론\n",
        "\n",
        "## 1. 정확도\n",
        "* ARIMA 기법 등 다른 통계적인 방법을 사용했을 때도 잘 예측하지만, 데이터가 방대해지면 시간이 오래걸림은 물론이고 정확도가 떨어지는 모습이 보이곤 한다. 마지막에 출력된 이미지를 보면 알 수 있다시피, 주기적인 모습은 어느정도 잘 맞추었으나 최대, 최소(Outliers)에 대한 부분은 잘 맞추지 못 한 것을 볼 수 있다.\n",
        "\n",
        "## 2. Scaler\n",
        "* 첫 시작 부분에서 Scaler는 scikit-learn의 MaxAbsScaler() 함수를 이용하였다. 아마 최대 최솟값이 -1 부터 1 사이로 지정되어있기 때문에 Original과 Prediction의 최대, 최소 부분이 차이가 나는 것으로 결론지었다."
      ]
    }
  ]
}